![journal-cms_subjects_2021-11_elife-sciences-neuroscience-illustration](https://github.com/user-attachments/assets/010103bf-ce47-42ed-b0f6-ced2ea5060f4)

This project implements a simple neural network with a tanh activation function in Python. The network uses random weight initialization and fixed biases to compute the output for a given input.

## How It Works
1. The input vector is passed through a hidden layer with tanh activation.
2. The output layer computes the final result without an activation function.
3. Weights are randomly initialized between -0.5 and 0.5.

## Code
The code is written in Python and uses NumPy for matrix operations. Check out the `Neuroscience project.py` file for the implementation.

## Author
[ Mahmoud Ayman ]
