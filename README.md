![journal-cms_subjects_2021-11_elife-sciences-neuroscience-illustration](https://github.com/user-attachments/assets/010103bf-ce47-42ed-b0f6-ced2ea5060f4)

This project implements a simple neural network with a tanh activation function in Python. The network uses random weight initialization and fixed biases to compute the output for a given input.

## How It Works
This Python script implements a simple artificial neural network with one hidden layer and two output neurons using the tanh activation function.

Weight Initialization: Weights are randomly chosen from [-0.5, 0.5].
Forward Propagation:
Computes hidden layer activation.
Computes output layer activation.
Error Calculation: Uses Mean Squared Error (MSE).
Output: Prints intermediate values, final network output, and error.

## Code
The code is written in Python and uses NumPy for matrix operations. Check out the `Neuroscience project.py` file for the implementation.

## Author
![Untitled-3](https://github.com/user-attachments/assets/ff01ecca-2503-4f91-825f-c9e641e795fd)
